{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import os.path\n",
    "from shutil import copyfile\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "# ----------------- import keras tools ----------------------\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Add, Reshape, Lambda, Concatenate, ZeroPadding2D\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from mnn.layers import CNNK1D, CNNR1D, CNNI1D, WaveLetC1D, InvWaveLetC1D\n",
    "from mnn.layers import CNNK2D\n",
    "from mnn.callback import SaveBestModel\n",
    "# ---------------- import python packages --------------------\n",
    "import argparse\n",
    "import h5py\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors\n",
    "from myplot import myPolarPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Scattering -- 2D')\n",
    "parser.add_argument('--epoch', type=int, default=40, metavar='N',\n",
    "                    help='# epochs for training in the each round (default: %(default)s)')\n",
    "parser.add_argument('--input-prefix', type=str, default='scafullV1N4', metavar='N',\n",
    "                    help='prefix of input data filename (default: %(default)s)')\n",
    "parser.add_argument('--alpha', type=int, default=40, metavar='N',\n",
    "                    help='number of channels for the depth for training (default: %(default)s)')\n",
    "parser.add_argument('--n-cnn', type=int, default=6, metavar='N',\n",
    "                    help='number CNN layers (default: %(default)s)')\n",
    "parser.add_argument('--n-cnn3', type=int, default=5, metavar='N',\n",
    "                    help='number CNN layers (default: %(default)s)')\n",
    "parser.add_argument('--noise', type=float, default=0, metavar='noise',\n",
    "                    help='noise on the measure data (default: %(default)s)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate for the first round (default: %(default)s)')\n",
    "parser.add_argument('--batch-size', type=int, default=32, metavar='N',\n",
    "                    help='batch size (default: %(default)s)')\n",
    "parser.add_argument('--verbose', type=int, default=2, metavar='N',\n",
    "                    help='verbose (default: %(default)s)')\n",
    "parser.add_argument('--output-suffix', type=str, default=None, metavar='N',\n",
    "                    help='suffix output filename(default: )')\n",
    "parser.add_argument('--percent', type=float, default=4./5., metavar='precent',\n",
    "                    help='percentage of number of total data(default: %(default)s)')\n",
    "parser.add_argument('--initialvalue', type=str, default=None, metavar='filename',\n",
    "                    help='filename storing the weights of the model (default: '')')\n",
    "parser.add_argument('--w-comp', type=int, default=1, metavar='N',\n",
    "                    help='window size of the compress(default: %(default)s)')\n",
    "parser.add_argument('--data-path', type=str, default='data/', metavar='string',\n",
    "                    help='data path (default: )')\n",
    "parser.add_argument('--log-path', type=str, default='logs/', metavar='string',\n",
    "                    help='log path (default: )')\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.output_suffix = 'ttT2'\n",
    "args.input_prefix = 'scafullV2N4'\n",
    "args.alpha = 40\n",
    "N_epoch = args.epoch\n",
    "alpha = args.alpha\n",
    "N_cnn = args.n_cnn\n",
    "N_cnn3 = args.n_cnn3\n",
    "lr = args.lr\n",
    "percent = args.percent\n",
    "batch_size = args.batch_size\n",
    "noise = args.noise\n",
    "noise_rate = noise / 100.\n",
    "input_prefix = args.input_prefix\n",
    "output_suffix = args.output_suffix\n",
    "data_path = args.data_path + '/'\n",
    "log_path = args.log_path + '/'\n",
    "print(f'N_epoch = {N_epoch}\\t alpha = {alpha}\\t (N_cnn, N_cnn3) = ({N_cnn}, {N_cnn3})\\t batch size = {batch_size}')\n",
    "print(f'lr = {lr:.2e}\\t percent = {percent}\\t noise = {noise}')\n",
    "print(f'input_prefix = {input_prefix}\\t output suffix = {output_suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(log_path):\n",
    "    os.mkdir(log_path)\n",
    "\n",
    "outputfilename  = log_path + 'S2d' + input_prefix[7:] \\\n",
    "    + 'Nc' + str(N_cnn) + 'Al' + str(alpha)\n",
    "if abs(int(noise) - noise) < 1.e-6:\n",
    "    outputfilename += \"Ns\" + str(int(noise))\n",
    "else:\n",
    "    outputfilename += \"Ns\" + str(noise)\n",
    "outputfilename += output_suffix or str(os.getpid())\n",
    "modelfilename   = outputfilename + '.h5'\n",
    "outputfilename += '.txt'\n",
    "log_os          = open(outputfilename, \"w+\")\n",
    "\n",
    "def output(obj):\n",
    "    print(obj)\n",
    "    log_os.write(str(obj)+'\\n')\n",
    "\n",
    "def outputnewline():\n",
    "    log_os.write('\\n')\n",
    "    log_os.flush()\n",
    "\n",
    "output(f'output filename is {outputfilename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenameIpt = data_path + input_prefix + '.h5'\n",
    "print('Reading data...')\n",
    "fin = h5py.File(filenameIpt, 'r')\n",
    "InputArray = fin['measure'][:]\n",
    "OutputArray = fin['coe'][:]\n",
    "Nsamples, Ns, Nd = InputArray.shape\n",
    "assert OutputArray.shape[0] == Nsamples\n",
    "Nsamples, Nt, Nr = OutputArray.shape\n",
    "Nd *= 2\n",
    "tmp = InputArray\n",
    "tmp2 = np.concatenate([tmp[:, Ns//2:Ns, :], tmp[:,0:Ns//2, :]], axis=1)\n",
    "InputArray = np.concatenate([tmp, tmp2], axis=2)\n",
    "InputArray = InputArray[:, :, Nd//4:3*Nd//4]\n",
    "print('Reading data finished')\n",
    "Nsamples, Ns, Nd = InputArray.shape\n",
    "print(f'Input shape is {InputArray.shape}')\n",
    "print(f'Output shape is {OutputArray.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.0, 1.0, Nt+1)[None, :]\n",
    "y = np.linspace(0, 1, Nr+1)\n",
    "y = y[:, None]\n",
    "x, y = np.broadcast_arrays(x, y)\n",
    "th = 2.0*np.pi*x\n",
    "rr = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "n_col = 3\n",
    "n_row = 1\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(n_row, n_col, 1, polar='True')\n",
    "plt.gca().pcolormesh(th, rr, np.transpose(OutputArray[k, :, :], (1,0)))\n",
    "plt.thetagrids([])\n",
    "plt.rgrids([])\n",
    "\n",
    "plt.subplot(n_row, n_col, 2)\n",
    "plt.imshow(OutputArray[k, :, :])\n",
    "plt.subplot(n_row, n_col, 3)\n",
    "plt.imshow(InputArray[k, :, :])\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output('alpha                   = %d\\t' % alpha)\n",
    "outputnewline()\n",
    "output('Input data filename     = %s' % filenameIpt)\n",
    "output(\"(Ns, Nd)                = (%d, %d)\" % (Ns, Nd))\n",
    "output(\"(Nt, Nr)                = (%d, %d)\" % (Nt, Nr))\n",
    "output(\"Nsamples                = %d\" % Nsamples)\n",
    "outputnewline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(Nsamples * percent)\n",
    "n_test  = min(max(n_train, 5000), Nsamples - n_train)\n",
    "BATCH_SIZE = batch_size\n",
    "n_valid = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1\n",
    "InputArray *= factor\n",
    "output(f'factor on the input data is {factor}')\n",
    "mean_out = 0\n",
    "max_out = np.amax(OutputArray)\n",
    "min_out = np.amin(OutputArray)\n",
    "pixel_max = max_out - min_out\n",
    "OutputArray /= 0.5 * pixel_max\n",
    "output(f'max / min of the output data are ({max_out:0.2f}, {min_out:0.2f})')\n",
    "max_out = np.amax(OutputArray)\n",
    "min_out = np.amin(OutputArray)\n",
    "pixel_max = max_out - min_out\n",
    "output(f'max / min of the output data are ({max_out:0.2f}, {min_out:0.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input  = (Ns, Nd)\n",
    "n_output = (Nt, Nr)\n",
    "output(\"[n_input, n_output] = [(%d,%d),  (%d,%d)]\" % (n_input + n_output))\n",
    "output(\"[n_train, n_test, n_valid]   = [%d, %d, %d]\" % (n_train, n_test, n_valid))\n",
    "output(\"batch size = %d\" % BATCH_SIZE)\n",
    "output(\"noise rate = %.2e\" % noise_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = InputArray[0:n_train, :, :]\n",
    "Y_train = OutputArray[0:n_train, :, :]\n",
    "X_test  = InputArray[n_train:(n_train+n_test), :, :]\n",
    "Y_test  = OutputArray[n_train:(n_train+n_test), :, :]\n",
    "\n",
    "# ---------- add noise on the input data ----------------------\n",
    "noiseTrain = np.random.randn(n_train, Ns, Nd) * noise_rate\n",
    "X_train = X_train * (1 + noiseTrain)\n",
    "noiseTest = np.random.randn(n_test, Ns, Nd) * noise_rate\n",
    "X_test = X_test * (1 + noiseTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_pixel = np.arange(1, 2*Nr+1, 2)\n",
    "\n",
    "def PSNR(img1, img2, pixel_max=1.0):\n",
    "    dimg = (img1 - img2) / pixel_max\n",
    "    mse = np.maximum(np.mean(dimg**2), 1.e-10)\n",
    "    return -10 * math.log10(mse)\n",
    "\n",
    "def PSNRs(imgs1, imgs2, pixel_max=1.0):\n",
    "    dimgs = (imgs1 - imgs2) / pixel_max\n",
    "    mse = np.maximum(np.mean(dimgs**2, axis=(1,2)), 1.e-10)\n",
    "    return -10 * np.mean(np.log10(mse))\n",
    "\n",
    "def test_data(model, X, Y):\n",
    "    Yhat = model.predict(X, n_valid)\n",
    "#     errs = np.linalg.norm((Yhat - Y) * weight_pixel, axis=(1, 2)) / np.linalg.norm((Y+mean_out) * weight_pixel, axis=(1, 2))\n",
    "#     return errs\n",
    "    return -PSNRs(Yhat, Y, pixel_max)\n",
    "\n",
    "def check_result(model):\n",
    "    return (test_data(model, X_train[0:n_valid, ...], Y_train[0:n_valid, ...]),\n",
    "            test_data(model, X_test[0:n_valid, ...], Y_test[0:n_valid, ...]))\n",
    "\n",
    "def test_data_mh(model_mh, X, Y):\n",
    "    Yhat = model_mh.predict(X, n_valid)\n",
    "#     errs1 = np.linalg.norm((Yhat[0] - Y) * weight_pixel, axis=(1, 2)) / np.linalg.norm((Y+mean_out)*weight_pixel, axis=(1, 2))\n",
    "#     errs2 = np.linalg.norm((Yhat[1] - Y) * weight_pixel, axis=(1, 2)) / np.linalg.norm((Y+mean_out)*weight_pixel, axis=(1, 2))\n",
    "#     return (errs1, errs2)\n",
    "    return (-PSNRs(Yhat[0], Y, pixel_max), -PSNRs(Yhat[1], Y, pixel_max))\n",
    "\n",
    "def check_result_mh(model_mh):\n",
    "    return test_data_mh(model_mh, X_test[0:n_valid, ...], Y_test[0:n_valid, ...])\n",
    "\n",
    "def splitScaling1D(X, alpha):\n",
    "    return Lambda(lambda x: x[:, :, alpha:2*alpha])(X)\n",
    "\n",
    "\n",
    "def splitWavelet1D(X, alpha):\n",
    "    return Lambda(lambda x: x[:, :, 0:alpha])(X)\n",
    "\n",
    "def Padding_x(x, s):\n",
    "    return K.concatenate([x[:, x.shape[1]-s:x.shape[1], ...], x, x[:, 0:s, ...]], axis=1)\n",
    "\n",
    "def __TriangleAdd(X, Y, alpha):\n",
    "    return K.concatenate([X[:, :, 0:alpha], X[:, :, alpha:2*alpha] + Y], axis=2)\n",
    "\n",
    "def TriangleAdd(X, Y, alpha):\n",
    "    return Lambda(lambda x: __TriangleAdd(x[0], x[1], alpha))([X, Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = 'period'\n",
    "w_comp = args.w_comp\n",
    "w_interp = w_comp\n",
    "L = math.floor(math.log2(Ns)) - 2  # number of levels\n",
    "m = Ns // 2**L     # size of the coarse grid\n",
    "m = 2 * ((m+1)//2) - 1\n",
    "w = 2 * 3    # support of the wavelet function\n",
    "n_b = 5      # bandsize of the matrix\n",
    "output(\"(L, m) = (%d, %d)\" % (L, m))\n",
    "\n",
    "Ipt = Input(shape=n_input)\n",
    "Ipt_c = CNNK1D(alpha, w_comp, activation='linear', bc_padding=bc)(Ipt)\n",
    "\n",
    "bt_list = (L+1) * [None]\n",
    "b = Ipt_c\n",
    "for ll in range(1, L+1):\n",
    "    bt = WaveLetC1D(2*alpha, w, activation='linear', use_bias=False)(b)\n",
    "    bt_list[ll] = bt\n",
    "    b = splitScaling1D(bt, alpha)\n",
    "\n",
    "# (b,t) --> d\n",
    "# d^L = A^L * b^L\n",
    "d = b\n",
    "for k in range(0, N_cnn):\n",
    "    d = CNNK1D(alpha, m, activation='relu', bc_padding='period')(d)\n",
    "\n",
    "# d = T^* * (D tb + (0,d))\n",
    "for ll in range(L, 0, -1):\n",
    "    d1 = bt_list[ll]\n",
    "    for k in range(0, N_cnn):\n",
    "        d1 = CNNK1D(2*alpha, n_b, activation='relu', bc_padding='period')(d1)\n",
    "\n",
    "#     d11 = splitWavelet1D(d1, alpha)\n",
    "#     d12 = splitScaling1D(d1, alpha)\n",
    "#     d12 = Add()([d12, d])\n",
    "#     d = Concatenate(axis=-1)([d11, d12])\n",
    "#     d = Lambda(lambda x: TriangleAdd(x[0], x[1], alpha))([d1, d])\n",
    "    d = TriangleAdd(d1, d, alpha)\n",
    "    d = InvWaveLetC1D(2*alpha, w//2, Nout=Nt//(2**(ll-1)), activation='linear', use_bias=False)(d)\n",
    "\n",
    "Img_c = d\n",
    "\n",
    "Img = CNNK1D(Nr, w_interp, activation='linear', bc_padding=bc)(Img_c)\n",
    "Img_p = Reshape(n_output+(1,))(Img)\n",
    "for k in range(0, N_cnn3-1):\n",
    "    Img_p = Lambda(lambda x: Padding_x(x, 1))(Img_p)\n",
    "    Img_p = ZeroPadding2D((0, 1))(Img_p)\n",
    "    Img_p = Conv2D(4, 3, activation='relu')(Img_p)\n",
    "    # Img_p = CNNK2D(4, 3, activation='relu', bc_padding=bc)(Img_p)\n",
    "\n",
    "Img_p = Lambda(lambda x: Padding_x(x, 1))(Img_p)\n",
    "Img_p = ZeroPadding2D((0, 1))(Img_p)\n",
    "Img_p = Conv2D(1, 3, activation='linear')(Img_p)\n",
    "# Img_p = CNNK2D(1, 3, activation='linear', bc_padding=bc)(Img_p)\n",
    "Opt = Reshape(n_output)(Img_p)\n",
    "Opt = Add()([Img, Opt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='figeit2dInv.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bs = []\n",
    "for bs in range(0, 5):\n",
    "    lr_bs.append([BATCH_SIZE * 2**bs, lr])\n",
    "\n",
    "for ll in range(1, 5):\n",
    "    lr_bs.append([BATCH_SIZE * 2**bs, lr * math.sqrt(0.1)**ll])\n",
    "    \n",
    "print(lr_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_bs2 = []\n",
    "for bs in range(3, 5):\n",
    "    lr_bs2.append([BATCH_SIZE * 2**bs, lr])\n",
    "\n",
    "for ll in range(1, 5):\n",
    "    lr_bs2.append([BATCH_SIZE * 2**bs, lr * math.sqrt(0.1)**ll])\n",
    "    \n",
    "print(lr_bs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multihead = Model(inputs=Ipt, outputs=[Opt, Img])\n",
    "model_multihead.compile(loss='mean_squared_error', optimizer='Nadam', loss_weights=[1., 1.])\n",
    "model_multihead.optimizer.schedule_decay = (0.004)\n",
    "output('number of params = %d' % model_multihead.count_params())\n",
    "save_best_model_mh = SaveBestModel(modelfilename, check_result=check_result_mh, period=1,\n",
    "                                   patience=10, output=output, test_weight=0., verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_epoch = 20\n",
    "n_epochs_pre = 0\n",
    "N_e = n_epochs_pre + 2 * N_epoch\n",
    "for b_s, l_r in lr_bs:\n",
    "    model_multihead.optimizer.lr = (l_r)\n",
    "    model_multihead.stop_training = False\n",
    "    model_multihead.fit(X_train, [Y_train, Y_train], batch_size=b_s, epochs=N_e,\n",
    "                        initial_epoch=n_epochs_pre, verbose=2, callbacks=[save_best_model_mh])\n",
    "    n_epochs_pre = N_e\n",
    "    N_e += N_epoch\n",
    "    model_multihead.load_weights(modelfilename, by_name=False)  # re-load the best model\n",
    "    save_best_model_mh.best_epoch_update = n_epochs_pre\n",
    "    Yhat_tmp = model_multihead.predict(X_test[0:100, ...], 100)\n",
    "    Yhat0 = Yhat_tmp[0]\n",
    "    Yhat1 = Yhat_tmp[1]\n",
    "#     print(f'PSNR for Opt: {PSNRs(Y_test[0:100, ...], Yhat0, pixel_max):.3g}')\n",
    "#     print(f'PSNR for Img: {PSNRs(Y_test[0:100, ...], Yhat1, pixel_max):.3g}')\n",
    "#     Yhat0 = np.maximum(np.minimum(Yhat0, max_out), min_out)\n",
    "#     Yhat1 = np.maximum(np.minimum(Yhat1, max_out), min_out)\n",
    "#     print(f'PSNR for Opt after post-precessing: {PSNRs(Y_test[0:100, ...], Yhat0, pixel_max):.3g}')\n",
    "#     print(f'PSNR for Img after post-precessing: {PSNRs(Y_test[0:100, ...], Yhat1, pixel_max):.3g}')\n",
    "    for idx in (0, 1):\n",
    "        datas = [Y_test[idx, ...], Yhat0[idx, ...], Yhat1[idx, ...]]\n",
    "        for k in range(len(datas)):\n",
    "            datas[k] = np.transpose(datas[k], [1, 0])\n",
    "\n",
    "        myPolarPlot(th, rr, datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_epochs_pre > 0:\n",
    "    model_multihead.load_weights(modelfilename, by_name=False)  # re-load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: final model\n",
    "model = Model(inputs=Ipt, outputs=Opt)\n",
    "model.compile(loss='mean_squared_error', optimizer='Nadam')\n",
    "model.optimizer.schedule_decay = (0.004)\n",
    "output('number of params = %d' % model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_epoch = 40\n",
    "save_best_model = SaveBestModel(modelfilename, check_result=check_result, period=1,\n",
    "                                    patience=10, output=output, test_weight=1., verbose=2)\n",
    "try:\n",
    "    n_epochs_pre\n",
    "except:\n",
    "    n_epochs_pre = 0\n",
    "\n",
    "N_e = n_epochs_pre + 2 * N_epoch\n",
    "for b_s, l_r in lr_bs2:\n",
    "    print(b_s, l_r)\n",
    "    model.optimizer.lr = (l_r)\n",
    "    model.stop_training = False\n",
    "    model.fit(X_train, Y_train, batch_size=b_s, epochs=N_e,\n",
    "              initial_epoch=n_epochs_pre, verbose=2, callbacks=[save_best_model])\n",
    "    n_epochs_pre = N_e\n",
    "    N_e += N_epoch\n",
    "    model.load_weights(modelfilename, by_name=False)\n",
    "    save_best_model.best_epoch_update = n_epochs_pre\n",
    "    Yhat = model.predict(X_test[0:100, ...], 100)\n",
    "#     print(f'PSNR for Opt: {PSNRs(Y_test[0:100, ...], Yhat, pixel_max):.3g}')\n",
    "#     Yhat = np.maximum(np.minimum(Yhat, max_out), min_out)\n",
    "#     print(f'PSNR for Opt after post-precessing: {PSNRs(Y_test[0:100, ...], Yhat, pixel_max):.3g}')\n",
    "    dY = Yhat - Y_test[0:100, ...]\n",
    "    for idx in (0, 1):\n",
    "        datas = [Y_test[idx, ...], Yhat[idx, ...], dY[idx, ...]]\n",
    "        for k in range(len(datas)):\n",
    "            datas[k] = np.transpose(datas[k], [1, 0])\n",
    "\n",
    "        myPolarPlot(th, rr, datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Yhat = model.predict(X_test[0:100, ...], 100)\n",
    "print(f'PSNR for Opt: {PSNRs(Y_test[0:100, ...], Yhat, pixel_max):.5g}')\n",
    "Yhat = np.maximum(np.minimum(Yhat, max_out), min_out)\n",
    "print(f'PSNR for Opt after post-precessing: {PSNRs(Y_test[0:100, ...], Yhat, pixel_max):.5g}')\n",
    "dY = np.absolute(Yhat - Y_test[0:100, ...])\n",
    "file_prefix = 'TTN4V1'\n",
    "for idx in (0, 1, 2, 4, 8):\n",
    "#     filename = 'figures/' + file_prefix + 'Sampling' + str(idx)\n",
    "#     myPolarPlot(th, rr, [np.transpose(Y_test[idx, ...], [1, 0])], figsize=(6,6), n_col=1, filename=filename + 'Y.png')\n",
    "#     myPolarPlot(th, rr, [np.transpose(Yhat[idx, ...], [1, 0])], figsize=(6,6), n_col=1, filename=filename + 'Ypred.png')\n",
    "#     myPolarPlot(th, rr, [np.transpose(dY[idx, ...], [1, 0])], figsize=(6,6), n_col=1, filename=filename + 'dY.png')\n",
    "    datas = [Y_test[idx, ...], Yhat[idx, ...], dY[idx, ...]]\n",
    "    for k in range(len(datas)):\n",
    "        datas[k] = np.transpose(datas[k], [1, 0])\n",
    "\n",
    "    myPolarPlot(th, rr, datas, colorbar='on', figsize=(12, 3))\n",
    "    print(f'PSNR of sample k is: {PSNR(Y_test[idx, ...], Yhat[idx, ...], pixel_max): .2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_file = 'rteNus2Nua0SymG9'\n",
    "n_test_new = 100\n",
    "filenameTest = data_path + test_file + '.h5'\n",
    "fin = h5py.File(filenameTest, 'r')\n",
    "TestInput = fin['measurement_diff_' + data_type][0:n_test_new, :, :]\n",
    "TestInput_bg = fin['measurement_' + data_type][:]\n",
    "TestOutput = fin['us'][0:n_test_new, :, :]\n",
    "TestOutput = np.transpose(TestOutput, [0, 2, 1])\n",
    "TestInput *= Input_factor\n",
    "TestInput_bg *= Input_factor\n",
    "TestOutput -= 1\n",
    "\n",
    "# ---------- add noise on the input data ----------------------\n",
    "ns_rate = noise_rate\n",
    "noiseTest = np.random.randn(n_test_new, Ns, Nd) * ns_rate\n",
    "TestInput = TestInput * (1 + noiseTest) + TestInput_bg * noiseTest\n",
    "\n",
    "TestYhat = model.predict(TestInput, min(n_test_new, 100))\n",
    "TestYhat = np.maximum(np.minimum(TestYhat, 1), 0)\n",
    "dY = TestYhat - TestOutput\n",
    "for idx in range(0, 10):\n",
    "    datas = [TestOutput[idx, ...], TestYhat[idx, ...], dY[idx, ...]]\n",
    "    for k in range(len(datas)):\n",
    "        datas[k] = np.transpose(datas[k], [1, 0])\n",
    "\n",
    "    myPolarPlot(th, r, datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
